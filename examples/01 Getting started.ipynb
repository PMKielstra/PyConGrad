{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d61eaac-cd7c-4622-aab7-22286fd79a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb36813-2e9b-4e8e-9598-280740e295d3",
   "metadata": {},
   "source": [
    "# Intro to PyConGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33c307-cf7d-4c94-b82b-1bcd217ce7f9",
   "metadata": {},
   "source": [
    "PyConGrad is a backend-agnostic conjugate gradient implementation.  For this tutorial we'll be using NumPy, but we could switch to CuPy or PyTorch in just a couple of lines or write a custom backend for almost any vector implementation that supports NumPy-like broadcasting and standard `+-*/` operator overloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a4aad6-0057-4f6e-9c47-7158af05a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.random.rand(N, N)\n",
    "A = X @ X.T + 0.001 * np.eye(100) # Guarantee that A is SPD\n",
    "batches = 2\n",
    "b = np.random.rand(N, batches) # By default, PyConGrad expects matrix equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30547c3e-5c9c-4ffa-9e0d-0e29e156d075",
   "metadata": {},
   "source": [
    "PyConGrad does not actually require explicitly-formed matrices.  It takes both its matvec and its (optional) preconditioner argument as functions that act on batched right-hand sides.  There's no PyConGrad reason why you can't have arbitrarily many batch dimensions; here we'll just use one.  The last dimension is always the vector dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9a893c-3ce1-4690-a178-6132d27c324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_batch(b):\n",
    "    return np.matmul(A, b)\n",
    "\n",
    "def diagonal_preconditioner(b):\n",
    "    return np.expand_dims(1 / np.diag(A), 1) * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b0523-ce56-4b28-b61d-50cf76f20043",
   "metadata": {},
   "source": [
    "Now we import `cg_batch` from the relevant backend.  For the purposes of documentation, we will specify every possible option here.  Mostly, you can use the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd369df-0ce6-446b-aa37-41a54574c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from congrad.numpy import cg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c6502c-f46b-47bc-8f93-ad0d45a3de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "020: 2.70353e+00 (1.21927e-03 seconds)\n",
      "040: 2.31055e+00 (2.52056e-03 seconds)\n",
      "060: 5.79571e+00 (3.60441e-03 seconds)\n",
      "080: 1.23486e+01 (4.71425e-03 seconds)\n",
      "100: 7.34654e+00 (5.85341e-03 seconds)\n",
      "120: 1.15786e+00 (6.91319e-03 seconds)\n",
      "140: 4.15936e-01 (7.96938e-03 seconds)\n",
      "160: 1.40437e-02 (9.02939e-03 seconds)\n",
      "180: 7.06661e-08 (1.00815e-02 seconds)\n",
      "Finished in 1.02799e-02 seconds after 183 iterations (1.78017e+04 iterations/second) with a maximum residual of 5.33382e-09.\n"
     ]
    }
   ],
   "source": [
    "solution, solve_info = cg_batch(A_batch,\n",
    "                                b,\n",
    "                                P=diagonal_preconditioner, # Preconditioner (default: None)\n",
    "                                x0=None,                   # Initial guess (default: None; will default to b in that case)\n",
    "                                rtol=1e-9,                 # Relative error tolerance, relative to |b| (default: 1e-3)\n",
    "                                atol=0,                    # Absolute error tolerance (default: 0)\n",
    "                                maxiter=1000,              # Maximum number of iterations, or None to have no maximum (default: 1000)\n",
    "                                warn_unconverged=True,     # Raise a warning if convergence fails (default: True)\n",
    "                                monitor=True,              # True to use the default monitor for solve progress, which prints the residual every 20 iters;\n",
    "                                                           # False or None to not use any monitor; or a Monitor subclass to use that monitor in particular.\n",
    "                                                           # (Default: None)\n",
    "                                flexible=False)            # Use \"flexible CG\" (Polak-Rib√®re rather than Fletcher-Reeves) (default: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f99622-a813-4dd9-b4b9-2bcd3009ccc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.541814113022449e-09\n",
      "3.620963438275689e-10\n"
     ]
    }
   ],
   "source": [
    "for batch in range(batches):\n",
    "    print(np.linalg.norm(A @ solution[:, batch] - b[:, batch]) / np.linalg.norm(b[:, batch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73884837-3df8-430c-9429-b9854b3d07a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'niter': 183,\n",
       " 'converged': True,\n",
       " 'residual': array([5.33381995e-09, 5.53794208e-10])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71560cdf-a287-4393-a4a7-954571d0f2c7",
   "metadata": {},
   "source": [
    "# Custom Backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd9f27-8714-4327-a70a-cace88a932e2",
   "metadata": {},
   "source": [
    "If we want to use a new vector backend, we can extend the `Backend` class to enable PyConGrad to talk to it.  For the purposes of the tutorial, we'll pretend we don't have a NumPy backend yet.\n",
    "\n",
    "All the methods we write here are defined as [static](https://docs.python.org/3/library/functions.html#staticmethod) in the base `Backend` class, so they don't take a `self` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1620d31-7f24-4eec-8b89-2a83ba0f06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from congrad import Backend, cg_batch_generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe39eca-200b-4f1b-a19a-110af51f3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumPyBackend(Backend):\n",
    "    def norm(X): # Norm in vector dimension\n",
    "        return np.linalg.norm(X, axis=-2)\n",
    "    \n",
    "    def dot(X, Y): # Dot product in vector dimension\n",
    "        XX = np.expand_dims(np.swapaxes(X, -1, -2), -2)\n",
    "        YY = np.expand_dims(np.swapaxes(Y, -1, -2), -1)\n",
    "        dot_prod = np.matmul(XX, YY)\n",
    "        return np.swapaxes(np.squeeze(dot_prod, axis=-1), -1, -2)\n",
    "            \n",
    "    def all_true(X): # Is every element of this boolean tensor true?  (Does not respect batching.)\n",
    "        return X.all()\n",
    "    \n",
    "    def max_vector_scalar(X, y): # Elementwise maximum of a tensor and a scalar.  (Does not respect batching.)\n",
    "        return np.maximum(X, y)\n",
    "    \n",
    "    def presentable_norm(residual): # Optional function that will be used to process residual norms before passing them to the monitor.\n",
    "                                    # Has no effect on the solve itself (unless you're using your monitor to alter the solve on the fly).\n",
    "        return np.max(residual).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a1ee9-7a81-4374-b95b-638922f821f8",
   "metadata": {},
   "source": [
    "Now that we have a backend, we create a CG function by using the class (not an instantiated object, since all the methods are static) to initialize a `cg_batch_generic` instance.  (Technically, `cg_batch_generic` is a class that implements `__call__`, meaning it can be called like a function.  In practice, you can think of `cg_batch_generic` as a function that maps backends to CG implementations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d0a593-a87c-4914-9e70-292b85932564",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_batch_numpy = cg_batch_generic(NumPyBackend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ade609-bc77-4e42-b32c-96a9114addef",
   "metadata": {},
   "source": [
    "This can then be used just like any other `cg_batch` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8d73b02-aa99-4ea7-918c-8e6f1bab90d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "020: 2.90157e+00 (1.75405e-03 seconds)\n",
      "040: 1.99742e+00 (3.43990e-03 seconds)\n",
      "060: 2.94993e+00 (4.06146e-03 seconds)\n",
      "080: 2.11710e+01 (4.70924e-03 seconds)\n",
      "100: 2.22407e+01 (5.53203e-03 seconds)\n",
      "120: 1.00542e+01 (6.13785e-03 seconds)\n",
      "140: 9.10128e-02 (6.77037e-03 seconds)\n",
      "160: 1.12286e-01 (7.39884e-03 seconds)\n",
      "Finished in 7.63106e-03 seconds after 167 iterations (2.18842e+04 iterations/second) with a maximum residual of 4.24306e-03.\n"
     ]
    }
   ],
   "source": [
    "solution, solve_info = cg_batch_numpy(A_batch, b, monitor=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
