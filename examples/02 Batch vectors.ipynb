{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e54a62-5da9-4191-a4b3-0ba2eba81730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from congrad import cg_batch_generic\n",
    "from congrad.torch import TorchBackend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e6850-6d0d-4e99-a3fd-a6c8932a3af0",
   "metadata": {},
   "source": [
    "This package's default backends all treat their right-hand side inputs as batches of *matrices* with arbitrarily many batch dimensions.  However, sometimes, we might want to have batches of *vectors* instead.  We can use some `reshape`s inside our batch matvec function to achieve similar behavior, or, if we need exact compatibility, we can write a custom backend as we do here.\n",
    "\n",
    "In fact, since all we need to do is take norms and dot products differently, we don't even extend the default `Backend` class.  Instead, we extend `TorchBackend` and change the two functions that matter to us.\n",
    "\n",
    "Alternatively, we could have extended `Backend`, which is only a little more work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff7326f-18df-4db6-be51-7f2d33db00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchVectorBackend(TorchBackend):\n",
    "    def norm(X):\n",
    "        return torch.linalg.vector_norm(X, dim=-1)\n",
    "\n",
    "    def dot(X, Y):\n",
    "        return torch.matmul(X.unsqueeze(-2), Y.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "cg_batch = cg_batch_generic(TorchVectorBackend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd9c432-feca-419f-8923-394d19230556",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "n_batches = 5\n",
    "\n",
    "X = torch.randn(N, N)\n",
    "A = X @ X.T + 0.001 * torch.eye(N)\n",
    "b = torch.randn(n_batches, N)\n",
    "\n",
    "def A_batch(x):\n",
    "    return torch.einsum(\"ij,bj->bi\", A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e4648c-6b78-4c09-9aec-70300e4df37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "020: 1.30577e+01 (1.93238e-03 seconds)\n",
      "040: 1.36897e+01 (3.00837e-03 seconds)\n",
      "060: 3.29272e+01 (3.99160e-03 seconds)\n",
      "080: 5.76979e+01 (4.97651e-03 seconds)\n",
      "100: 5.30374e+01 (5.94234e-03 seconds)\n",
      "120: 2.88872e+01 (6.94442e-03 seconds)\n",
      "140: 6.34805e+00 (7.92074e-03 seconds)\n",
      "160: 6.68151e-05 (8.89492e-03 seconds)\n",
      "Finished in 9.03511e-03 seconds after 162 iterations (1.79301e+04 iterations/second) with a maximum residual of 6.72177e-06.\n"
     ]
    }
   ],
   "source": [
    "solution, info = cg_batch(A_batch, b, rtol=1e-6, monitor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7458e1f2-623a-45a3-8b24-d624c957ee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5848e-06)\n",
      "tensor(2.9012e-07)\n",
      "tensor(2.0339e-07)\n",
      "tensor(3.4547e-07)\n",
      "tensor(1.0593e-06)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_batches):\n",
    "    print(torch.linalg.vector_norm(A @ solution[i] - b[i]) / torch.linalg.vector_norm(b[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
